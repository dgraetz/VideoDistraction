
# Bonus: Predicting distractibility of videos



```{r eval = FALSE}

embed <- readRDS("get_videoembeddings/results/embeddings.RDS")

c <- coefficients(logistic_img_content$lr$finalModel, logistic_img_content$lr$bestTune$lambda) %>% as.matrix()

columns <- colnames(embed)[grepl("V[[:digit:]]", colnames(embed))]

for (i in 1:length(columns)){
  embed[,columns[i]] <- embed[,columns[i]] * c[rownames(c) == columns[i]]
}

distractibility = rowSums(embed[grepl("V[[:digit:]]", colnames(embed))])

embed <- cbind(distractibility, embed)
distract <- embed %>%
  arrange(distractibility) %>%
  group_by(Video) %>%
  slice(1) %>% 
  arrange(distractibility)

image_paths <- paste0("G:/My Drive/Studium/University of Oregon/Cognitive Dynamics Lab/Experiments/video_exp/videostimuli/videoframes_HQ/",distract$Video, "/", distract$Img)

library(grid)
library(jpeg)
imgs_lower <- list()

for(i in 1:9){
  img <- readJPEG(image_paths[i])
  img <- rasterGrob(img, interpolate=TRUE)
  imgs_lower[[i]] <- ggplot()+
    annotation_custom(img)+
    xlim(0, 1)+
    ylim(0, 1)+
    coord_fixed()+
    theme_void()+
    theme(plot.margin = margin(0,0,0,0))
  print(i)
}

imgs_upper <- list()

for(i in (length(image_paths)-9):length(image_paths)){
  img <- readJPEG(image_paths[i])
  img <- rasterGrob(img, interpolate=TRUE)
  imgs_upper[[i]] <- ggplot()+
    annotation_custom(img)+
    xlim(0, 1)+
    ylim(0, 1)+
    coord_fixed()+
    theme_void()+
    theme(plot.margin = margin(0,0,0,0))
  print(i)
}

ggarrange(plotlist = imgs_lower, nrow = 4, ncol = 5)
ggarrange(plotlist = imgs_upper, nrow = 4, ncol = 5)

```
